{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IllK3qv-KFw8"
      },
      "outputs": [],
      "source": [
        "# data extraction\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# In notebooks/Colab, install BEFORE importing fitz on a fresh runtime:\n",
        "# !pip install PyMuPDF\n",
        "\n",
        "def filename_creator(sus_report):\n",
        "    \"\"\"Extract company name and year from filename\"\"\"\n",
        "    name = os.path.splitext(os.path.basename(sus_report))[0]\n",
        "    parts = re.split(r\"[_\\-\\s]\", name)\n",
        "    year = next((p for p in parts if re.fullmatch(r\"\\d{4}\", p)), \"Unknown\")\n",
        "    companies = {\"BP\", \"TOTAL\", \"SHELL\",\"EXXON\",\"CHEVRON\"}  # extend later\n",
        "    company = next((p.upper() for p in parts if p.upper() in companies), \"Unknown\")\n",
        "    return company, year\n",
        "\n",
        "def extract_pages_fitz(file_path, sparse_threshold=50):\n",
        "    \"\"\"\n",
        "    Extract text from PDF using PyMuPDF with proper page numbers.\n",
        "    Keeps ALL pages (including short ones), and flags sparse pages.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "\n",
        "    try:\n",
        "        doc = fitz.open(file_path)\n",
        "\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc[page_num]\n",
        "            text = page.get_text(\"text\").strip()  # Extract + trim\n",
        "\n",
        "            char_count = len(text)\n",
        "            word_count = len(text.split()) if text else 0\n",
        "\n",
        "            data.append({\n",
        "                \"page_number\": page_num + 1,  # 1-indexed\n",
        "                \"text\": text,\n",
        "                \"char_count\": char_count,\n",
        "                \"word_count\": word_count,\n",
        "                \"is_sparse\": char_count < sparse_threshold\n",
        "            })\n",
        "\n",
        "        doc.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "def process_all_pdfs(folder_path, sparse_threshold=50):\n",
        "    \"\"\"Process all PDFs in folder and create dataset\"\"\"\n",
        "    all_data = []\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.lower().endswith(\".pdf\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            print(f\"ðŸ“„ Processing: {filename}\")\n",
        "\n",
        "            company, year = filename_creator(filename)\n",
        "            pages_data = extract_pages_fitz(file_path, sparse_threshold=sparse_threshold)\n",
        "\n",
        "            for page_info in pages_data:\n",
        "                all_data.append({\n",
        "                    \"Company\": company,\n",
        "                    \"Year\": year,\n",
        "                    \"Page\": page_info[\"page_number\"],\n",
        "                    \"Text\": page_info[\"text\"],\n",
        "                    \"Char_Count\": page_info[\"char_count\"],\n",
        "                    \"Word_Count\": page_info[\"word_count\"],\n",
        "                    \"Is_Sparse\": page_info[\"is_sparse\"],\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(all_data)\n",
        "\n",
        "# Usage\n",
        "folder_path = \"/content/\"\n",
        "df = process_all_pdfs(folder_path, sparse_threshold=50)\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(\"BIGOIL_pages4.csv\", index=False)\n",
        "print(f\"âœ… Extracted {len(df)} pages from {df['Company'].nunique()} companies\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chunking the writing blocks\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/BIGOIL_pages4 (1).csv\")\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"tagger\"])\n",
        "\n",
        "def split_long_text(\n",
        "    text,\n",
        "    target_words=400,\n",
        "    max_words=450\n",
        "):\n",
        "    \"\"\"\n",
        "    Split text into sentence-based chunks.\n",
        "    Sentences are accumulated until ~target_words.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_word_count = 0\n",
        "\n",
        "    for sent in sentences:\n",
        "        sent_word_count = len(sent.split())\n",
        "\n",
        "        if current_word_count + sent_word_count <= target_words:\n",
        "            current_chunk.append(sent)\n",
        "            current_word_count += sent_word_count\n",
        "        else:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = [sent]\n",
        "            current_word_count = sent_word_count\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "new_rows = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    text = str(row[\"Text\"])\n",
        "    word_count = row[\"Word_Count\"]\n",
        "\n",
        "    # Case 1: short rows â†’ keep as-is\n",
        "    if word_count <= 450:\n",
        "        new_row = row.to_dict()\n",
        "        new_row[\"Subpage\"] = float(row[\"Page\"])  # e.g. 40.0\n",
        "        new_rows.append(new_row)\n",
        "        continue\n",
        "\n",
        "    # Case 2: long rows â†’ split\n",
        "    chunks = split_long_text(text)\n",
        "\n",
        "    for i, chunk in enumerate(chunks, start=1):\n",
        "        new_row = row.to_dict()\n",
        "        new_row[\"Text\"] = chunk\n",
        "        new_row[\"Word_Count\"] = len(chunk.split())\n",
        "        new_row[\"Char_Count\"] = len(chunk)\n",
        "        new_row[\"Subpage\"] = float(f\"{int(row['Page'])}.{i}\")  # 40.1, 40.2, ...\n",
        "        new_rows.append(new_row)\n",
        "\n",
        "df_chunked = pd.DataFrame(new_rows)\n",
        "\n",
        "print(\"Original rows:\", len(df))\n",
        "print(\"New rows after chunking:\", len(df_chunked))\n",
        "\n",
        "df_chunked.to_csv(\n",
        "    \"/content/BIGOIL_pages_sentence_chunked.csv\",\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print(\"âœ… Saved: BIGOIL_pages_sentence_chunked.csv\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FEJNLRxBKNTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering and clearing\n",
        "\n",
        "#cleaning the dataset\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# 1. Load CHUNKED dataset (important)\n",
        "df = pd.read_csv(\"/content/BIGOIL_pages_sentence_chunked.csv\")\n",
        "\n",
        "# 3. Light cleaning\n",
        "def light_clean(text):\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', str(text))\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = text.replace('-', ' ')\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip().lower()\n",
        "\n",
        "df[\"Text\"] = df[\"Text\"].apply(light_clean)\n",
        "\n",
        "# 4. Green / environmental lexicon\n",
        "greenwashing_keywords = [\n",
        "    \"environment\", \"environmental\", \"biodiversity\", \"nature\", \"ecosystem\", \"ecosystems\",\n",
        "    \"species\", \"ocean\", \"oceans\", \"forest\", \"forests\", \"water\", \"deforest\", \"deforestation\",\n",
        "    \"endangered\", \"protect\", \"protection\", \"agriculture\", \"agricultural\", \"continent\",\n",
        "    \"hemisphere\", \"climate\", \"climatic\", \"warming\", \"global warming\", \"extreme weather\",\n",
        "    \"cyclone\", \"cyclones\", \"biology\", \"biologist\", \"chemistry\", \"chemical\", \"chemicals\",\n",
        "    \"sustainable\", \"sustainability\", \"sustain\", \"green\", \"greenwashing\", \"greenhouse\",\n",
        "    \"renewable\", \"renewables\", \"energy\", \"clean energy\", \"net zero\", \"carbon neutrality\",\n",
        "    \"carbon neutral\", \"transition\", \"transitions\", \"transitioning\", \"decarbon\",\n",
        "    \"decarbonize\", \"decarbonise\", \"decarbonization\", \"decarbonisation\",\n",
        "    \"alternative energy\", \"alternative fuel\", \"green investment\", \"green investments\",\n",
        "    \"circular economy\", \"low carbon\", \"climate positive\", \"energy efficiency\",\n",
        "    \"energy efficient\", \"dual challenge\", \"ambition\", \"ambitions\", \"commitment\",\n",
        "    \"commitments\", \"leadership\", \"vision\", \"visionary\", \"carbon\", \"carbon dioxide\", \"co2\",\n",
        "    \"ghg\", \"greenhouse gas\", \"greenhouse gases\", \"emission\", \"emissions\", \"methane\", \"ch4\",\n",
        "    \"footprint\", \"carbon footprint\", \"effluent\", \"effluents\", \"pollutant\", \"pollutants\",\n",
        "    \"pollution\", \"hazardous\", \"hazard\", \"contaminated\", \"contamination\", \"disposal\",\n",
        "    \"waste disposal\", \"flaring\", \"gas flaring\", \"abatement\", \"carbon abatement\",\n",
        "    \"carbon capture\", \"carbon sink\", \"carbon offset\", \"carbon offsets\", \"carbon tax\",\n",
        "    \"carbon pricing\", \"carbon price\", \"solar\", \"solar power\", \"solar energy\", \"wind\",\n",
        "    \"wind energy\", \"wind power\", \"hydrogen\", \"green hydrogen\", \"biofuel\", \"biofuels\",\n",
        "    \"biomass\", \"battery\", \"batteries\", \"geothermal\", \"electric vehicle\",\n",
        "    \"electric vehicles\", \"ev\", \"evs\", \"hydropower\", \"hydroelectric\", \"clean tech\",\n",
        "    \"material\", \"materials\", \"metric\", \"metrics\", \"target\", \"targets\", \"tonnes\", \"tons\",\n",
        "    \"scope 1\", \"scope 2\", \"scope 3\", \"baseline\", \"benchmark\", \"benchmarks\", \"reduction\",\n",
        "    \"reductions\", \"increase\", \"increases\", \"ogci\", \"ipcc\", \"paris agreement\", \"paris\",\n",
        "    \"kyoto\", \"unfccc\", \"1.5Â°c\", \"2Â°c\", \"two degrees\", \"one point five degrees\"\n",
        "]\n",
        "\n",
        "# 5. Regex pattern\n",
        "pattern = re.compile(\n",
        "    r'\\b(?:' + '|'.join(re.escape(word) for word in greenwashing_keywords) + r')\\b',\n",
        "    flags=re.IGNORECASE\n",
        ")\n",
        "\n",
        "# 6. Filter rows\n",
        "df_filtered = df[df[\"Text\"].apply(lambda x: bool(pattern.search(x)))]\n",
        "\n",
        "# 7. Save\n",
        "df_filtered.to_csv(\"/content/cleaned_oil_chunked.csv\", index=False)\n",
        "\n",
        "print(f\"âœ… Done. {len(df_filtered)} rows retained.\")\n"
      ],
      "metadata": {
        "id": "82ARXAEIKbwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the new character counts etc\n",
        "\n",
        "df = pd.read_csv(\"/content/cleaned_oil_chunked.csv\")\n",
        "\n",
        "df[\"Word_Count\"] = df[\"Text\"].str.split().str.len()\n",
        "df[\"Char_Count\"] = df[\"Text\"].str.len()\n",
        "\n",
        "df.to_csv(\"/content/cleaned_oil_chunked_fixed.csv\", index=False)\n",
        "print(\"âœ… Saved: cleaned_oil_chunked_fixed.csv\", \"Rows:\", len(df))"
      ],
      "metadata": {
        "id": "UWPczIntKrSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Â lemmantion etc\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# 1. Load the CLEAN & CHUNKED dataset\n",
        "df = pd.read_csv(\"/content/clean&fresh_oil_.csv\")\n",
        "\n",
        "# 2. Load spaCy with only what we need\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
        "\n",
        "# 3. Lemmatization function\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(str(text))\n",
        "    lemmas = [\n",
        "        token.lemma_\n",
        "        for token in doc\n",
        "        if not token.is_punct and not token.is_space\n",
        "    ]\n",
        "    return \" \".join(lemmas)\n",
        "\n",
        "# 4. Apply lemmatization\n",
        "df[\"lemmatized_text\"] = df[\"Text\"].apply(lemmatize_text)\n",
        "\n",
        "# 5. Save output\n",
        "df.to_csv(\"/content/clean_fresh_oil_lemmatized.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Lemmatization complete. Saved as clean_fresh_oil_lemmatized.csv\")\n"
      ],
      "metadata": {
        "id": "uyIP8lJJK0u_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# basic sentiment analysis\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# 1. Load your latest dataset\n",
        "df = pd.read_csv(\"/content/clean_fresh_oil_lemmatized.csv\")\n",
        "\n",
        "# 2. Load RoBERTa-based sentiment model\n",
        "sentiment_model = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"siebert/sentiment-roberta-large-english\"\n",
        ")\n",
        "\n",
        "# 3. Sentiment function (safe truncation)\n",
        "def get_sentiment(text):\n",
        "    result = sentiment_model(\n",
        "        str(text),\n",
        "        truncation=True\n",
        "    )[0]\n",
        "    return result[\"label\"], float(result[\"score\"])\n",
        "\n",
        "# 4. Apply sentiment analysis\n",
        "df[[\"sentiment_label\", \"sentiment_score\"]] = df[\"Text\"].apply(\n",
        "    lambda x: pd.Series(get_sentiment(x))\n",
        ")\n",
        "\n",
        "# 5. Save output\n",
        "output_path = \"/content/clean_fresh_oil_with_sentiment.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(\"âœ… RoBERTa sentiment complete!\")\n",
        "print(\"Saved as:\", output_path)\n",
        "\n",
        "# Quick sanity check\n",
        "print(df[[\"Text\", \"sentiment_label\", \"sentiment_score\"]].head(5))\n"
      ],
      "metadata": {
        "id": "70jjhz7tLHG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# ClimateBERT Full Inference Pipeline (ALL MODELS)\n",
        "# =====================================================\n",
        "\n",
        "\n",
        "# 2. Imports\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# =====================================================\n",
        "# 3. Load dataset\n",
        "# =====================================================\n",
        "\n",
        "INPUT_PATH = \"/content/clean_fresh_oil_with_sentiment.csv\"\n",
        "OUTPUT_PATH = \"/content/clean_fresh_oil_with_all_climatebert.csv\"\n",
        "\n",
        "df = pd.read_csv(INPUT_PATH)\n",
        "\n",
        "print(\"Loaded dataset:\", df.shape)\n",
        "\n",
        "# =====================================================\n",
        "# 4. Helper function to run any ClimateBERT classifier\n",
        "# =====================================================\n",
        "\n",
        "def run_climate_model(model_name, prefix, df, batch_size=16):\n",
        "    \"\"\"\n",
        "    Runs a HuggingFace text-classification model on df['Text']\n",
        "    and appends label, score, and full probability distribution.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\nRunning model: {model_name}\")\n",
        "\n",
        "    clf = pipeline(\n",
        "        \"text-classification\",\n",
        "        model=model_name,\n",
        "        device=0,                 # GPU\n",
        "        truncation=True,\n",
        "        return_all_scores=True\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    scores = []\n",
        "    probs  = []\n",
        "\n",
        "    texts = df[\"Text\"].astype(str).tolist()\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        outputs = clf(batch)\n",
        "\n",
        "        for out in outputs:\n",
        "            # Sort predictions by confidence\n",
        "            out_sorted = sorted(out, key=lambda x: x[\"score\"], reverse=True)\n",
        "            best = out_sorted[0]\n",
        "\n",
        "            labels.append(best[\"label\"])\n",
        "            scores.append(float(best[\"score\"]))\n",
        "            probs.append({d[\"label\"]: float(d[\"score\"]) for d in out})\n",
        "\n",
        "    df[f\"{prefix}_label\"] = labels\n",
        "    df[f\"{prefix}_score\"] = scores\n",
        "    df[f\"{prefix}_probs\"] = probs\n",
        "\n",
        "    print(f\"âœ… Finished: {prefix}\")\n",
        "\n",
        "# =====================================================\n",
        "# 5. Run ALL ClimateBERT models\n",
        "# =====================================================\n",
        "\n",
        "# 5.1 Specificity (specific vs non-specific)\n",
        "run_climate_model(\n",
        "    model_name=\"climatebert/distilroberta-base-climate-specificity\",\n",
        "    prefix=\"specificity\",\n",
        "    df=df\n",
        ")\n",
        "\n",
        "# 5.2 Climate detector (climate vs not)\n",
        "run_climate_model(\n",
        "    model_name=\"climatebert/distilroberta-base-climate-detector\",\n",
        "    prefix=\"climate_detector\",\n",
        "    df=df\n",
        ")\n",
        "\n",
        "# 5.3 Commitment framing\n",
        "run_climate_model(\n",
        "    model_name=\"climatebert/distilroberta-base-climate-commitment\",\n",
        "    prefix=\"commitment\",\n",
        "    df=df\n",
        ")\n",
        "\n",
        "# 5.4 TCFD category classification\n",
        "run_climate_model(\n",
        "    model_name=\"climatebert/distilroberta-base-climate-tcfd\",\n",
        "    prefix=\"tcfd\",\n",
        "    df=df\n",
        ")\n",
        "\n",
        "# 5.5 Climate-specific sentiment\n",
        "run_climate_model(\n",
        "    model_name=\"climatebert/distilroberta-base-climate-sentiment\",\n",
        "    prefix=\"climate_sentiment\",\n",
        "    df=df\n",
        ")\n",
        "\n",
        "# =====================================================\n",
        "# 6. Save final dataset\n",
        "# =====================================================\n",
        "\n",
        "df.to_csv(OUTPUT_PATH, index=False)\n",
        "\n",
        "print(\"\\nðŸŽ‰ ALL CLIMATEBERT MODELS COMPLETED\")\n",
        "print(\"Saved to:\", OUTPUT_PATH)\n",
        "print(\"Final shape:\", df.shape)\n"
      ],
      "metadata": {
        "id": "W1yvYyPZLKhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# addition of buzzword scoring and fuutre scoring\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('/content/clean_fresh_oil_with_all_climatebert.csv')\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# --- POS-based future verbs (as lemmas) ---\n",
        "future_verbs = {\n",
        "    \"will\", \"shall\", \"aim\", \"aiming\", \"plan\", \"planning\", \"target\", \"aspire\",\n",
        "    \"commit\", \"committed\", \"pledge\", \"pledged\", \"pledging\", \"vow\", \"promise\",\n",
        "    \"seek\", \"intend\", \"intends\", \"hope\", \"envision\", \"anticipate\", \"anticipates\", \"forecast\",\n",
        "    \"forecasting\", \"project\", \"projects\", \"projected\", \"look\", \"working\", \"moving\", \"set\"\n",
        "}\n",
        "\n",
        "# --- Phrase-based future expressions (not captured as single tokens by spaCy) ---\n",
        "future_phrases = [\n",
        "    \"going to\", \"seek to\", \"intend to\", \"set out to\", \"hope to\", \"looking to\",\n",
        "    \"working to\", \"working towards\", \"moving towards\", \"transition plan\",\n",
        "    \"path to net zero\", \"by 2030\", \"by 2050\",\n",
        "    \"weâ€™re helping to save the planet\"\n",
        "]\n",
        "\n",
        "# --- Buzzword lexicon for greenwashing ---\n",
        "greenwashing_lexicon = [\n",
        "    \"green\", \"clean\", \"cleaner\", \"cleanest\", \"efficient\", \"sustainable\",\n",
        "    \"sustainability\", \"eco-friendly\", \"environmentally friendly\", \"earth-friendly\",\n",
        "    \"eco-conscious\", \"natural\", \"non-toxic\", \"organic\", \"ethical\", \"biodegradable\",\n",
        "    \"carbon neutral\", \"climate neutral\", \"carbon offsets\", \"carbon offset\", \"carbon credits\",\n",
        "    \"low carbon\", \"fuels of tomorrow\", \"resilient hydrocarbons\", \"energy in progress\",\n",
        "    \"transformation\", \"energy transition\", \"transition\", \"beyond petroleum\",\n",
        "    \"emissions intensity\", \"locally grown\", \"sustainably sourced\", \"eco-safe\",\n",
        "    \"eco-preferred\", \"cfc-free\", \"chlorofluorocarbon-free\", \"renewable natural gas\", \"rng\",\n",
        "    \"carbon capture and storage\", \"ccs\", \"e-fuels\", \"synthetic fuels\", \"synth-fuels\",\n",
        "    \"carbon-neutral e-fuels\", \"drive carbon neutral\"\n",
        "]\n",
        "\n",
        "# --- Compile regex patterns for phrase matching ---\n",
        "future_phrase_pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(phrase) for phrase in future_phrases) + r')\\b', flags=re.IGNORECASE)\n",
        "buzzword_pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(term) for term in greenwashing_lexicon) + r')\\b', flags=re.IGNORECASE)\n",
        "\n",
        "# --- POS-based future score function ---\n",
        "def pos_future_score(text):\n",
        "    if pd.isnull(text) or text.strip() == \"\":\n",
        "        return 0.0\n",
        "\n",
        "    doc = nlp(text)\n",
        "    token_count = len(doc)\n",
        "    future_count = 0\n",
        "\n",
        "    for i, token in enumerate(doc):\n",
        "        lemma = token.lemma_.lower()\n",
        "        pos = token.pos_\n",
        "\n",
        "        # Match modal verbs like \"will\", \"shall\" only when followed by a verb\n",
        "        if lemma in {\"will\", \"shall\"} and token.tag_ == \"MD\":\n",
        "            if i + 1 < token_count and doc[i + 1].pos_ == \"VERB\":\n",
        "                future_count += 1\n",
        "            else:\n",
        "                future_count += 1  # accept anyway to stay inclusive\n",
        "\n",
        "        # Match intention/commitment words when used as verbs or auxiliaries\n",
        "        elif lemma in future_verbs and pos in {\"VERB\", \"AUX\"}:\n",
        "            future_count += 1\n",
        "\n",
        "    # Also match multi-word future phrases via regex\n",
        "    future_count += len(future_phrase_pattern.findall(text.lower()))\n",
        "\n",
        "    return future_count / token_count if token_count > 0 else 0.0\n",
        "\n",
        "# --- Buzzword score function (greenwashing density) ---\n",
        "def buzzword_score(text):\n",
        "    if pd.isnull(text) or text.strip() == \"\":\n",
        "        return 0.0\n",
        "    word_count = len(text.split())\n",
        "    if word_count == 0:\n",
        "        return 0.0\n",
        "    buzzword_matches = buzzword_pattern.findall(text.lower())\n",
        "    return len(buzzword_matches) / word_count\n",
        "\n",
        "# --- Apply both scoring functions to your DataFrame ---\n",
        "df[\"future_score_pos\"] = df[\"Text\"].apply(pos_future_score)\n",
        "df[\"buzzword_score\"] = df[\"Text\"].apply(buzzword_score)\n",
        "\n",
        "# --- Save new dataset with both scores included ---\n",
        "df.to_csv(\"/content/oil_reports_with_future_and_buzzword_scores.csv\", index=False)\n",
        "\n",
        "print(\"âœ… POS-based future score and buzzword score added!\")\n",
        "print(\"ðŸ“„ Saved as: oil_reports_with_future_and_buzzword_scores.csv\")\n"
      ],
      "metadata": {
        "id": "I1B3SfO0LNd3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}